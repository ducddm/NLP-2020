{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_1_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxGKXm4rcA8Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Name: Đỗ Đăng Minh Đức\n",
        "Student ID: USTHBI8-042\n",
        "Email address: ducddm.b8042@st.usth.edu.vnvn\n",
        "\n",
        "\n",
        "Programming Assignment 1: Bigram Language Models\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yqjr08lMcjK5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xYVZ6lgcdKJ",
        "colab_type": "code",
        "outputId": "362638f1-3a2b-4a6a-f4dd-1c279824a2be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "source": [
        "!rm -f wiki-en-train.word\n",
        "!wget https://raw.githubusercontent.com/neubig/nlptutorial/master/data/wiki-en-train.word\n",
        "    \n",
        "!rm -f wiki-en-test.word\n",
        "!wget https://raw.githubusercontent.com/neubig/nlptutorial/master/data/wiki-en-test.word\n",
        "\n",
        "!rm -f 02-train-input.txt\n",
        "!wget https://raw.githubusercontent.com/neubig/nlptutorial/master/test/02-train-input.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-07 11:39:09--  https://raw.githubusercontent.com/neubig/nlptutorial/master/data/wiki-en-train.word\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 203886 (199K) [text/plain]\n",
            "Saving to: ‘wiki-en-train.word’\n",
            "\n",
            "\rwiki-en-train.word    0%[                    ]       0  --.-KB/s               \rwiki-en-train.word  100%[===================>] 199.11K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2020-02-07 11:39:09 (4.16 MB/s) - ‘wiki-en-train.word’ saved [203886/203886]\n",
            "\n",
            "--2020-02-07 11:39:12--  https://raw.githubusercontent.com/neubig/nlptutorial/master/data/wiki-en-test.word\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 26989 (26K) [text/plain]\n",
            "Saving to: ‘wiki-en-test.word’\n",
            "\n",
            "wiki-en-test.word   100%[===================>]  26.36K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-02-07 11:39:12 (2.12 MB/s) - ‘wiki-en-test.word’ saved [26989/26989]\n",
            "\n",
            "--2020-02-07 11:39:15--  https://raw.githubusercontent.com/neubig/nlptutorial/master/test/02-train-input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12 [text/plain]\n",
            "Saving to: ‘02-train-input.txt’\n",
            "\n",
            "02-train-input.txt  100%[===================>]      12  --.-KB/s    in 0s      \n",
            "\n",
            "2020-02-07 11:39:15 (3.73 MB/s) - ‘02-train-input.txt’ saved [12/12]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AU2UGgCwcLjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Part 1.1: Function train_bigram()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rogJfAnpcb6w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def train_bigram(train_file, model_file):\n",
        "    \"\"\"Train trigram language model and save to model file\n",
        "    \"\"\"\n",
        "    counts = defaultdict(int)  # count the n-gram\n",
        "    context_counts = defaultdict(int)   # count the context\n",
        "    total_count = 0\n",
        "    with open(train_file) as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line == '':\n",
        "                continue\n",
        "            words = line.split()\n",
        "            words.append('</s>')\n",
        "            words.insert(0, '<s>')\n",
        "\n",
        "  \n",
        "    \n",
        "            for i in range(1, len(words)):  # Note: starting at 1, after <s>\n",
        "                # TODO: Write code to count bigrams and their contexts\n",
        "                # YOUR CODE HERE\n",
        "\n",
        "                counts[words[i-1]+ \" \"+ words[i]] += 1\n",
        "                context_counts[words[i-1]] +=1\n",
        "\n",
        "                counts[words[i]] +=1\n",
        "                context_counts[\"\"] +=1\n",
        "                total_count +=1\n",
        "\n",
        "\n",
        "                pass\n",
        "\n",
        "    # Save probabilities to the model file            \n",
        "    with open(model_file, 'w') as fo:\n",
        "        for ngram, count in counts.items():\n",
        "            # TODO: Write code to calculate probabilities of n-grams \n",
        "            # (unigrams and bigrams)\n",
        "            # Hint: probabilities of n-grams will be calculated by their counts\n",
        "            # divided by their context's counts.\n",
        "            # probability = counts[ngram]/context_counts[context]\n",
        "            # After calculating probabilities, we will save ngram and probability\n",
        "            # to the file in the format:\n",
        "            # ngram<tab>probability\n",
        "\n",
        "            # YOUR CODE HERE\n",
        "\n",
        "            words = ngram.split()\n",
        "          \n",
        "    \n",
        "            if (len(words)==1) : probability = counts[ngram] / total_count\n",
        "            else : probability = counts[ngram] / context_counts[words[0]]\n",
        "\n",
        "            fo.write('%s\\t%f\\n' % (ngram, probability))\n",
        "\n",
        "            pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Qo6pSyLc6Uh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Test the train_bigram function on a small data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kal6o_QcdCQh",
        "colab_type": "code",
        "outputId": "280b4a04-19f5-4634-e661-a2d75055cd8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "source": [
        "train_bigram('02-train-input.txt', '02-train-answer.txt')\n",
        "!cat 02-train-answer.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<s> a\t1.000000\n",
            "a\t0.250000\n",
            "a b\t1.000000\n",
            "b\t0.250000\n",
            "b c\t0.500000\n",
            "c\t0.125000\n",
            "c </s>\t1.000000\n",
            "</s>\t0.250000\n",
            "b d\t0.500000\n",
            "d\t0.125000\n",
            "d </s>\t1.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Dhd8WYkdI7a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Part 1.2: function load_bigram_model "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_c8FBThdQwS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_bigram_model(model_file):\n",
        "    \"\"\"Load the model file\n",
        "\n",
        "    Args:\n",
        "        model_file (str): Path to the model file\n",
        "    \n",
        "    Returns:\n",
        "        probs (dict): Dictionary object that map from ngrams to their probabilities\n",
        "    \"\"\"\n",
        "    probs = {}\n",
        "    with open(model_file, 'r') as f:\n",
        "        for line in f:\n",
        "            # TODO: From each line split ngram, probability\n",
        "            # and then update probs\n",
        "            \n",
        "            # YOUR CODE HERE\n",
        "\n",
        "            line = line.strip()\n",
        "          \n",
        "            \n",
        "            w,p = line.split(\"\\t\")\n",
        "            probs[w] = float(p)\n",
        "            pass\n",
        "    return probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOHTakDHdXsS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Test the function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJuAjy4FdbG6",
        "colab_type": "code",
        "outputId": "e65efe3d-5190-4649-dd1b-a350bf05cdda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "source": [
        "load_bigram_model('02-train-answer.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'</s>': 0.25,\n",
              " '<s> a': 1.0,\n",
              " 'a': 0.25,\n",
              " 'a b': 1.0,\n",
              " 'b': 0.25,\n",
              " 'b c': 0.5,\n",
              " 'b d': 0.5,\n",
              " 'c': 0.125,\n",
              " 'c </s>': 1.0,\n",
              " 'd': 0.125,\n",
              " 'd </s>': 1.0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZECEGjDdhYR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Part 2: Evaluating Bigram Language Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4cn91r7dl85",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Function test_bigram"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCwISBUNdpOy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "\n",
        "def test_bigram(test_file, model_file, lambda2=0.95, lambda1=0.95, N=1000000):\n",
        "    W = 0 # Total word count\n",
        "    H = 0\n",
        "    probs = load_bigram_model(model_file)\n",
        "    with open(test_file, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line == '':\n",
        "                continue\n",
        "            words = line.split()\n",
        "            words.append('</s>')\n",
        "            words.insert(0, '<s>')\n",
        "            for i in range(1, len(words)):  # Note: starting at 1, after <s>\n",
        "                # TODO: Write code to calculate smoothed unigram probabilties\n",
        "                # and smoothed bigram probabilities\n",
        "                # You should use calculate p1 as smoothed unigram probability\n",
        "                # and p2 as smoothed bigram probability\n",
        "              \n",
        "\n",
        "                # YOUR CODE HERE\n",
        "\n",
        "                if probs.get(words[i]) : p1 = lambda1 * probs.get(words[i]) + (1-lambda1) / N\n",
        "                else : p1 = lambda1 * 0 + (1-lambda1)/N\n",
        "\n",
        "\n",
        "                if (probs.get(words[i-1]+\" \"+words[i])) : p2 = lambda2 * probs.get(words[i-1]+\" \"+words[i]) + (1-lambda2) * p1\n",
        "                else : p2 = lambda2 * 0 + (1-lambda2) * p1\n",
        "                    \n",
        "                # END OF YOUR CODE\n",
        "\n",
        "                W += 1  # Count the words\n",
        "                H += -math.log2(p2) # We use logarithm to avoid underflow\n",
        "    H = H/W\n",
        "    P = 2**H\n",
        "\n",
        "    print(\"Entropy: {}\".format(H))\n",
        "    print(\"Perplexity: {}\".format(P))\n",
        "\n",
        "    return P"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFwM6XVtd1ej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Calculate on the Wikipedia data."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHmpJyk_d5d6",
        "colab_type": "code",
        "outputId": "0c7dcfd2-d1d5-48c5-f564-24747bb224cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "train_bigram('wiki-en-train.word', 'bigram_model.txt')\n",
        "test_bigram('wiki-en-test.word', 'bigram_model.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Entropy: 11.284333504778214\n",
            "Perplexity: 2494.151707389251\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2494.151707389251"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    }
  ]
}